!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Page</title>
	<link rel="icon" href="../../ressources/images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="../../ressources/styles/style.css">
    <script src="https://kit.fontawesome.com/cb30d04475.js" crossorigin="anonymous"></script>
</head>
<body>
    <nav>
		<div class="logo">
			<img src="../../ressources/images/Logo.png" alt="Victor's Logo">
			<span class="portfolio-text">Victor's Portfolio</span>
		</div>
	   	<div class="nav-container">
			<div class="social-media-links">
				<a href="https://github.com/victordechaisemartin" target="_blank" rel="noopener noreferrer">
					<i class="fab fa-github"></i>
				</a>
				<a href="https://linkedin.com/in/victor-de-chaisemartin/" target="_blank" rel="noopener noreferrer">
					<i class="fab fa-linkedin-in"></i>
				</a>
	    	</div>
        	<div class="nav-links">
            		<li><a href="../../index.html">Home</a></li>
            		<li><a href="../index.html" class="active">Projects</a></li>
					<li><a href="../../about/index.html">About me</a></li>
                    <li><a href="../../contact/index.html">Contact</a></li>
       		</div>
		</div>
    </nav>
    <div class="banner">
        <img src="../../ressources/images/project4/project4.png" alt="Banner Image">
    </div>
    <div class="project-content">
        <div class="projects-title">
            <h1>Toxic Comment Classification</h1>
            <p>20/01/2024</p>
        </div>
        <div class="text">
            <p>Welcome to my Toxic Comment Classification project!</p>
        </div>
        <p><br></p>
        <p>View the full project on <a href="https://github.com/victordechaisemartin/toxic-comment-classification" target="_blank" class="github-link">GitHub</a>.</p>

        <div class="context">
            <h2>Context</h2>
            <p>
                This project aims to create a model that can classify comments from online platforms into various toxicity categories such as toxic, severe toxic, obscene, threat, insult, identity hate, etc... 
                The purpose of this project is to improve online conversations by filtering out inappropriate comments. <br><br>
                We will begin by exploring the dataset, performing data preprocessing and visualization, followed by training different machine learning models to classify the toxic comments accurately.
            </p>
        </div>
        <div class="data-analysis">
            <h2>Data Analysis</h2>
            <p>
                The data analysis phase is crucial in understanding the nuances and patterns within the comments. 
                Through various visualizations, we aim to gain insights into the distribution and nature of toxicity across different categories. 
                These visualizations help to identify trends and commonalities within the data, providing a solid foundation for the subsequent modeling phase.
            </p>
            <!-- Plot 1  -->
            <div class="plot-item">
                <img src="../../ressources/images/project4/plot1.png" alt="Distribution of Categories">
                <figcaption>Fig 1: Distribution of Categories</figcaption>
                <p>
                    The bar chart illustrates the distribution of comments across various categories. 
                    It is evident that some categories have a higher frequency, indicating a greater number of comments flagged as such. 
                    This uneven distribution poses challenges in model training due to class imbalance.
                    A closer look at the 'toxicity' category reveals that it encompasses the majority of comments, suggesting a need for robust filtering mechanisms in online platforms to maintain healthy interactions.
                </p>
            </div>

            <div class="plot-item">
                <img src="../../ressources/images/project4/plot2.png" alt="Comment Word Count Distribution">
                <figcaption>Fig 2: Comment Word Count Distribution</figcaption>
                <p>
                    This histogram showcases the word count distribution across the comments.
                    Most comments are brief, but there is a long tail of longer comments which may contain more complex patterns of toxicity.
                    The distribution's skewness towards shorter comment lengths indicates that toxic content is not always verbose, and even succinct comments can contain harmful material.
                </p>
            </div>

            <div class="plot-item">
                <img src="../../ressources/images/project4/plot3.png" alt="Correlation Matrix">
                <figcaption>Fig 3: Correlation Matrix</figcaption>
                <p>
                    The correlation matrix here helps us understand the relationship between different categories of toxicity. 
                    Some categories show a higher correlation, suggesting that comments often contain multiple forms of toxicity.
                    Notably, the matrix indicates a significant overlap between 'insult' and 'obscene' comments, which could suggest a commonality in the language used for these types of toxicity.
                </p>
            </div>

            <div class="plot-item">
                <img src="../../ressources/images/project4/plot4.png" alt="Word Cloud for all Comments">
                <figcaption>Fig 4: Word Cloud for all Comments</figcaption>
                <p>
                    This word cloud provides a visual representation of the most frequent words in all toxic comments. 
                    The size of each word indicates its frequency, offering insights into common themes and terms used.
                    Dominant words such as 'hate' and 'stupid' in this word cloud underscore the aggressive tone prevalent in toxic comments.
                </p>
            </div>

            <div class="plot-item">
                <img src="../../ressources/images/project5/plot5.png" alt="Word Cloud for LGBTQ Comments">
                <figcaption>Fig 4: Word Cloud for LGBTQ Comments</figcaption>
                <p>
                    Similarly, this word cloud reflects the vocabulary of another category, enabling us to compare the language used across different forms of toxicity.
                    The prevalence of identity-based terms suggests that a significant portion of toxic comments may be targeting specific groups or individuals.
                </p>
            </div>
        </div>
        <div class="data-processing">
            <h2>Data Processing</h2>
            <p>

            </p>
            <pre><code class="language-python">

            </code></pre>
        <div class="data-modelling">
            <h2>Data Modelling</h2>
            <p>

            </p>     
        </div>
        <div class="results">
            <h2>Conclusion</h2>
            <p>

            </p>
        </div>
    </div>
    <div class="related-projects">
        <h2>Read Next</h2>
        <div class="project-list">

            <!-- Project 1 -->
            <div class="project-card">
                <a href="../Project 1/index.html">
                    <img src="../../ressources/images/project1/project1.png" alt="Traffic Forecasting">
                    <div class="project-info">
                        <h3>Traffic Forecasting</h3>
                        <p>10/12/2023</p>
                    </div>
                </a>
            </div>
    
            <!-- Project 3 -->
            <div class="project-card">
                <a href="../Project 3/index.html">
                    <img src="../../ressources/images/project3/project3.png" alt="Tumor Detection">
                    <div class="project-info">
                        <h3>Tumor Detection</h3>
                        <p>20/01/2024</p>
                    </div>
                </a>
            </div>    
        </div>
    </div>
    <footer>
        <p>&copy; 2023 Victor de Chaisemartin. All rights reserved.</p>
    </footer>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
</body>
</html>