!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Page</title>
	<link rel="icon" href="../../ressources/images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="../../ressources/styles/style.css">
    <script src="https://kit.fontawesome.com/cb30d04475.js" crossorigin="anonymous"></script>
</head>
<body>
    <nav>
		<div class="logo">
			<img src="../../ressources/images/Logo.png" alt="Victor's Logo">
			<span class="portfolio-text">Victor's Portfolio</span>
		</div>
	   	<div class="nav-container">
			<div class="social-media-links">
				<a href="https://github.com/victordechaisemartin" target="_blank" rel="noopener noreferrer">
					<i class="fab fa-github"></i>
				</a>
				<a href="https://linkedin.com/in/victor-de-chaisemartin/" target="_blank" rel="noopener noreferrer">
					<i class="fab fa-linkedin-in"></i>
				</a>
	    	</div>
        	<div class="nav-links">
            		<li><a href="../../index.html">Home</a></li>
            		<li><a href="../index.html" class="active">Projects</a></li>
					<li><a href="../../about/index.html">About me</a></li>
                    <li><a href="../../contact/index.html">Contact</a></li>
       		</div>
		</div>
    </nav>
    <div class="banner">
        <img src="../../ressources/images/project3/project3.png" alt="Banner Image">
    </div>
    <div class="project-content">
        <div class="projects-title">
            <h1>Tumor Detection</h1>
            <p>20/01/2024</p>
        </div>
        <div class="text">
            <p>Welcome to my Tumor Detection project!</p>
        </div>
        <p><br></p>
        <p>View the full project on <a href="https://github.com/victordechaisemartin/tumor_detection" target="_blank" class="github-link">GitHub</a>.</p>

        <div class="context">
            <h2>Context</h2>
            <p>
                The objective of this project is to train a model using deep learning techniques to recognize tumors on scans of metastatic tissue (or in simpler terms, images of cells). The field of digital pathology is developing rapidly, following recent advancements in microscopic imaging hardware that allow digitizing glass slides into whole-slide images. This facilitates image analysis and allows us to use deep learning models to automate diagnostics tasks.<br><br>
                We will start by analyzing th PCAM dataset and extracting useful information. We will then dig into model architectures that are powerful for analyzing images and we will test their accuracy on our data.
            </p>
        </div>
        <div class="data-analysis">
            <h2>Data Analysis</h2>
            <p>The PatchCamelyon benchmark (PCAM) consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annoted with a binary label indicating presence of metastatic tissue.<br><br>
                Let's plot ten 5 images containing tumors (label : 1) and 5 without (label : 0) to see if we can see the difference.
            </p>
            <!-- Plot 1  -->
            <div class="plot-item">
                <img src="../../ressources/images/project3/plot1.png" alt="Plot of 10 images with their labels">
                <figcaption>Fig 1: Plot of 10 images with their labels</figcaption>
                <p>
                    As you can see, each slide shows a magnified view of tissue samples stained to highlight cellular structures and anomalies.
                    The top row, labeled '0', represents biopsy slides that have been identified as benign, showing no signs of cancer. The bottom row, labeled '1', contains images of biopsy slides that are malignant, indicating the presence of cancer.<br><br>
                    Next, we can plot the ditribution of our label data.
                </p>
            </div>

            <!-- Plot 2 -->
            <div class="plot-item">
                <img src="../../ressources/images/project3/plot2.png" alt="Data Label Distribution">
                <figcaption>Fig 2: Data Label Distribution</figcaption>
                <p>The training set distribution is visualized on the left with a light blue bar, representing the frequency of each class label, whereas the validation set on the right is depicted with two gold bars. 
                    The consistent height of the bars across both training and validation sets suggests an even class distribution, which is essential for training unbiased and well-generalized machine learning models. <br><br>
                    This balanced distribution is particularly important in medical datasets where the accurate detection of conditions, such as cancer, is critical.
                </p>
            </div>
        </div>
        <div class="data-processing">
            <h2>Data Processing</h2>
            <p>Now that we have analyzed our data, we can start the pre-processing phase.</p>
            <p><br></p>
            <p>The data is composed of images in the same format. Therefore, there is no need for any formatting steps. 
                However, to improve the quality of our predictions, we can apply some image transformations.
            </p>
            <pre><code class="language-python">
    class PCAMDataset(Dataset):
    def __init__(self, data, labels, train, transform = None):
        super(PCAMDataset, self).__init__()
        self.data = data
        self.labels = labels
        self.train = train
        self.transform = transform

        if self.train:
            self.augmentation = transforms.Compose([
                transforms.ToPILImage(),
                transforms.RandomHorizontalFlip(),
                transforms.RandomVerticalFlip(),
                transforms.ToTensor()
            ])
        else:
            self.augmentation = transforms.Compose([
                transforms.ToPILImage(),
                transforms.ToTensor()
            ])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        image = self.data[idx]
        label = self.labels[idx]

        image = self.augmentation(image)

        if self.transform:
            image = self.transform(image)

        return image, label
            </code></pre>
            <p>Within this custom dataset class, specific transformations are applied to the data during preprocessing.
                Before transforming our data, we need to turn our images from arrays to PIL images. 
                This is necessary because the subsequent augmentation operations are implemented to work on PIL Image objects, which are a standard format for image manipulation in Python.
                We then apply horizontal and vertical random flips to augment our data.
            </p>
            <div class="plot-item">
                <img src="../../ressources/images/project3/plot3.png" alt="Explaining Data Augmentation">
                <figcaption>Fig 3: Data Augmentation with Flipping Example</figcaption>
                <p><strong>Data augmentation: </strong>
                    The concept of data augmentation with flipping is particularly important in the context of medical imaging. 
                    It artificially expands the training dataset by creating modified versions of images, which helps the model to generalize better by learning from a more diverse set of examples.
                    Both images above are identical. However, one has been flipped to create a new image with different pixel parameters.
                </p>
            </div>
        </div>
        <div class="data-modelling">
            <h2>Data Modelling</h2>
            <p>Data modelling is where the magic happens. In this section, I share the methodologies and algorithms used to predict traffic patterns.
                First, we will try different hand-made convolutional architectures to see the impact of each hyperparameters.
                For the first model, we will build a simple 2 layer convolutional network, then we will add a third layer and compare the results.
            </p>
            <pre><code class="language-python">
    class ConvNet_2L(nn.Module):
        def __init__(self):  # Assuming binary classification; adjust num_classes as needed
            super(ConvNet_2L, self).__init__()
            # Convolutional layers
            self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
            self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)

            # Pooling layer
            self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

            # Fully connected layer
            self.fc1 = nn.Linear(64 * 48 * 48, 2)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
            x = self.pool(x)

            # Flatten the tensor for the fully connected layer correctly
            x = x.view(x.size(0), -1)

            x = self.fc1(x)
            return x           
            </code></pre>
            <p>
                In each layer, we add relu activation for non linearity. Wa also added a pooling layer for dimension reduction after the second convolutional layer.
                The method used here is Maximum Pooling, it works by taking the maximum value over a specified window, thereby highlighting the most prominent features within that window
                Finally, we use a fully connected linear layer to reduce our output to the number of classes we have. In this case we only have 2. We have to be careful when defining the input size for the fully connected layer as the original input size has changed with the convolutional layers.<br><br>
                For the second model, we will try adding a third convolutional layer to see the differences in performance.
            </p>
            <pre><code class="language-python"></code>
    class ConvNet_3L(nn.Module):
        def __init__(self):
            super(ConvNet_3L, self).__init__()
            # Convolutional layers
            self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
            self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
            self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)

            # Pooling layer
            self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)

            # Fully connected layer
            self.fc1 = nn.Linear(64 * 24 * 24, 2)


        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
            x = self.pool(x)
            x = F.relu(self.conv3(x))
            x = self.pool(x)

            # Flatten the tensor for the fully connected layer
            x = x.view(-1, 64 * 24 * 24)

            x = self.fc1(x)

            return x
            </code></pre>
            <p>
                The architecture is similar as the previous mode. The only difference is that we now have two pooling layers, one after the second convolutional layer and one after the third.
                The input size for the fully connected layer is also different because of the third layer. <br><br>
                We will also try another different architecture for this project : the VGG16.
            </p>
            <pre><code class="language-python"></code>
    class ConvNet_VGG16(nn.Module):
        def __init__(self, num_classes=2):
            super(ConvNet_VGG16, self).__init__()
            self.features = nn.Sequential(
                # Block 1
                nn.Conv2d(3, 64, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),

                # Block 2
                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),

                # Block 3
                nn.Conv2d(128, 256, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),

                # Block 4
                nn.Conv2d(256, 512, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),

                # Block 5
                nn.Conv2d(512, 512, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),
            )

            self.avgpool = nn.AdaptiveAvgPool2d((7, 7))

            self.classifier = nn.Sequential(
                nn.Linear(512 * 7 * 7, 4096),
                nn.ReLU(inplace=True),
                nn.Dropout(),
                nn.Linear(4096, 4096),
                nn.ReLU(inplace=True),
                nn.Dropout(),
                nn.Linear(4096, num_classes),
            )

        def forward(self, x):
            x = self.features(x)
            x = self.avgpool(x)
            x = x.view(x.size(0), -1)
            x = self.classifier(x)
            return x
            </code></pre>
            <p>
                This architecture is caracterised by stacking multiple 3x3 convolutional layers sequentially.
                The key is repetition and depth. For the VGG16, we have 16 layers : 13 convolutional layers and 3 fully connected layers.
                The layers are organised into blocks of 2-4 convolutional layers, each composed of an additional pooling layer to reduce the spatial size of the representation and to increase the receptive field.
                This architecture is good for image classification as it is uniformal. It is also deep enough to learn a wide range of features and also had an increased receptive field.<br><br>
                Finally, the last architecture we will try out is the ResNet. 
            </p>
            <pre><code class="language-python"></code>
    class ResidualBlock(nn.Module):
        def __init__(self, in_channels, out_channels, stride=1, downsample=None):
            super(ResidualBlock, self).__init__()
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
            self.bn1 = nn.BatchNorm2d(out_channels)
            self.relu = nn.ReLU(inplace=True)
            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
            self.bn2 = nn.BatchNorm2d(out_channels)
            self.downsample = downsample
    
        def forward(self, x):
            residual = x
            out = self.conv1(x)
            out = self.bn1(out)
            out = self.relu(out)
            out = self.conv2(out)
            out = self.bn2(out)
            if self.downsample:
                residual = self.downsample(x)
            out += residual
            out = self.relu(out)
            return out
    
    class ConvNet_ResNetLike(nn.Module):
        def __init__(self):
            super(ConvNet_ResNetLike, self).__init__()
            self.in_channels = 64
            self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1)
            self.bn1 = nn.BatchNorm2d(self.in_channels)
            self.relu = nn.ReLU(inplace=True)
            self.layer1 = self._make_layer(64, 2, stride=1)
            self.layer2 = self._make_layer(128, 2, stride=2)
            self.layer3 = self._make_layer(256, 2, stride=2)
            self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(256, 2)
    
        def _make_layer(self, out_channels, blocks, stride):
            downsample = None
            if (stride != 1) or (self.in_channels != out_channels):
                downsample = nn.Sequential(
                    nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride),
                    nn.BatchNorm2d(out_channels)
                )
            layers = []
            layers.append(ResidualBlock(self.in_channels, out_channels, stride, downsample))
            self.in_channels = out_channels
            for _ in range(1, blocks):
                layers.append(ResidualBlock(out_channels, out_channels))
            return nn.Sequential(*layers)
    
        def forward(self, x):
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.adaptive_pool(x)
            x = x.view(x.size(0), -1)
            x = self.fc(x)
            return x
        </code></pre>
        <p>
            This architecture is well-known for addressing the vanishing gradient problem. Indeed, by learning residual mappings instead of direct mappings, you allow gradients to flow through the network more effectively.
            This leads to improved performances on complex image recognition tasks.
            As you can see here, the layers take into input the result of the convolutional layers as well as the previous input of the block.
            You can also compile the residual blocks for more depth. <br><br>
            To compare the performances of these three different models, we can plot the training and validation loss on 10 epochs.
        </p>
        <div class="plot-item">
            <img src="../../ressources/images/project3/plot4.png" alt="Training/Validation Loss">
            <figcaption>Fig 4: Training/Validation Loss</figcaption>
            <p>
                Here you can see that the model that performs the best is the three layer convolutional layers. 
                This was expected as the VGG16 and ResNet are more complex and very deep. 
                Therefore, they require more data to be very effective. 
                A simpler architecture like the 3 layer CNN is sufficient for the data we have.
                We can also plot the confusion matrix for all 4 models to better understand our results.
            </p>
        </div>
        <div class="plot-item">
            <img src="../../ressources/images/project3/plot5.png" alt="Confusion Matrix for our models">
            <figcaption>Fig 5: Confusion Matrix for our models</figcaption>
            <p>
                These results confirm what we previously said.
                However, to improve the accuracy of our VGG16 and ResNet models, we can try transfer learning.
                This is very useful and often leads to the best results in Kaggle Competitions.
                Transfer Learning consists in using a pretrained model and train it on our data.
                It will have already learned on a huge database so it will already be powerful but it will also be specialised for our task.
            </p>
        </div>     
        </div>
        <div class="results">
            <h2>Conclusion</h2>
            <p>

            </p>
        </div>
    </div>
    <div class="related-projects">
        <h2>Read Next</h2>
        <div class="project-list">

            <!-- Project 1 -->
            <div class="project-card">
                <a href="../Project 1/index.html">
                    <img src="../../ressources/images/project1/project1.png" alt="Traffic Forecasting">
                    <div class="project-info">
                        <h3>Traffic Forecasting</h3>
                        <p>10/12/2023</p>
                    </div>
                </a>
            </div>
    
            <!-- Project 2 -->
            <div class="project-card">
                <a href="../Project 2/index.html">
                    <img src="../../ressources/images/project2/project2.png" alt="Diabetes Analysis">
                    <div class="project-info">
                        <h3>I used Data Analysis tools to extract insights on my diabetes</h3>
                        <p>02/06/2023</p>
                    </div>
                </a>
            </div>    
        </div>
    </div>
    <footer>
        <p>&copy; 2023 Victor de Chaisemartin. All rights reserved.</p>
    </footer>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
</body>
</html>